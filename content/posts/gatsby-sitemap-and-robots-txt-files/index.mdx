---
title: Gatsby robots.txt and sitemap.xml files
description: How to generate robots.txt and sitemap.xml files for Gatsby site
datePublished: 2021-08-18
dateModified: 2021-08-18
image: ./cover.jpg
imageAlt: Husky doggo
---

import TableOfContents from "../../../src/components/TableOfContents"
import Intro from "../../../src/components/Intro"
import {
  SeriesIntro,
  CompletedProjectedLinks,
} from "../../components/gatsby-seo-series"
import { ProjectTreePartFour } from "./components"

<Intro>

<SeriesIntro part="4" />

In part #4, we will learn about `sitemap.xml` and `robots.txt` files.

<CompletedProjectedLinks />

Take a few moments to get familiar with the project structure (click on the folder to expand it).

<ProjectTreePartFour />

</Intro>

<TableOfContents
  items={props.tableOfContentsItems}
  ids={props.tableOfContentsHeaderIds}
/>

## sitemap.xml

`sitemap.xml` is a way to provide more information for search engines about pages on your website. Sitemap is a file that
lists URLs for a website along with additional metadata about each URL.

To generate `sitemap.xml` install [gatsby-plugin-sitemap](https://github.com/gatsbyjs/gatsby/tree/master/packages/gatsby-plugin-sitemap)
plugin.

```sh
yarn add gatsby-plugin-sitemap
```

Add `gatsby-plugin-sitemap` in `gatsby-config.js`.

```js fileName=gatsby-config.js
// ✂️
module.exports = {
  plugins: [
    `gatsby-plugin-sitemap`,
    // ✂️
  ],
}
```

`gatsby-plugin-sitemap` by default expects site URL from `siteMetadata.siteUrl`, so we dont need to do anything else.
It's important to note that this plugin generates sitemap files only in `production` mode. To validate generated `sitemap.xml`
file you will need to build and serve the site.

```
gatsby build && gatsby serve
```

Navigate to `http://localhost:9000/sitemap/sitemap-index.xml` and you should see:

```xml
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <sitemap>
    <loc>https://gatsby-seo.netlify.app/sitemap/sitemap-0.xml</loc>
  </sitemap>
</sitemapindex>
```

What you see is not the actual sitemap, it's a sitemap index page. According to Google
[single `sitemap.xml`](https://developers.google.com/search/docs/advanced/sitemaps/build-sitemap#sitemapformat)
is limited to 50MB (uncompressed) and 50,000 URLs. Originally these limits were created to
ensure that your web server won't be overloaded by serving large files to search engines.
Sitemap index page solves this issue, by letting you break down your sitemap into smaller
pieces. For those of you who is interested to know what will be the max amount of URLs
submitted with a single sitemap index check out this [StackOverflow answer](https://stackoverflow.com/a/34972582/3614631).

To open the actual `sitemap.xml` copy the URL from the `loc` node and open it in browser `http://localhost:9000/sitemap/sitemap-0.xml`
(production URL was replaced with `localhost`).

```xml
<urlset
  xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
  xmlns:news="http://www.google.com/schemas/sitemap-news/0.9"
  xmlns:xhtml="http://www.w3.org/1999/xhtml"
  xmlns:image="http://www.google.com/schemas/sitemap-image/1.1"
  xmlns:video="http://www.google.com/schemas/sitemap-video/1.1"
>
  <url>
    <loc>https://gatsby-seo.netlify.app/</loc>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>https://gatsby-seo.netlify.app/blog/siberian-husky/</loc>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  // ✂️
</urlset>
```

As it turns out `changefreq` and `priority` are not used by Google any more. We can safely remove this bloat from
`sitemap.xml` and add whats actually important - last modified date `lastmod`.

Update `gatsby-config.js` file:

```jsx fileName=gatsby-config.js
const path = require("path")
const siteMetadata = require("./site-metadata")
const slashify = require("./src/helpers/slashify")

// ✂️
const {
  NODE_ENV,
  SITE_URL,
  URL: NETLIFY_SITE_URL = SITE_URL,
  DEPLOY_PRIME_URL: NETLIFY_DEPLOY_URL = NETLIFY_SITE_URL,
  CONTEXT: NETLIFY_ENV = NODE_ENV,
} = process.env
const isNetlifyProduction = NETLIFY_ENV === `production`
const siteUrl = isNetlifyProduction ? NETLIFY_SITE_URL : NETLIFY_DEPLOY_URL

module.exports = {
  siteMetadata: {
    ...siteMetadata,
    siteUrl,
  },
  plugins: [
    // ✂️
    {
      resolve: `gatsby-plugin-sitemap`,
      options: {
        query: `{
          allMdx {
            nodes {
              frontmatter {
                published
                modified
              }
              fields {
                slug
              }
            }
          }
        }`,
        resolveSiteUrl: () => siteUrl,
        resolvePages: ({ allMdx: { nodes: mdxNodes } }) => {
          const { pages } = siteMetadata
          const blogPathName = pages.blog.pathName

          const allPages = Object.values(pages).reduce((acc, { pathName }) => {
            if (pathName) {
              acc.push({ path: slashify(pathName) })
            }
            return acc
          }, [])

          const allArticles = mdxNodes.map(
            ({ frontmatter: { published, modified }, fields: { slug } }) => ({
              path: slashify(blogPathName, slug),
              lastmod: modified ? modified : published,
            })
          )

          return [...allPages, ...allArticles]
        },
        serialize: ({ path: url, lastmod }) => ({
          url,
          lastmod,
        }),
      },
    },
}
```

We fetch and format (thanks to [`slashify` function](/blog/gatsby-node-and-config-files/#slashify-code-block)) all page
and article URLs. Then we fetch last modified date for each article and create a sitemap entry for each article. When
article doesn't have last modified date we fallback to the published date.

With the above changes your `sitemap.xml` should look like this:

```xml
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
  xmlns:news="http://www.google.com/schemas/sitemap-news/0.9"
  xmlns:xhtml="http://www.w3.org/1999/xhtml"
  xmlns:image="http://www.google.com/schemas/sitemap-image/1.1"
  xmlns:video="http://www.google.com/schemas/sitemap-video/1.1">
  <url>
    <loc>https://gatsby-seo-draft.netlify.app/</loc>
  </url>
  <url>
    <loc>https://gatsby-seo-draft.netlify.app/blog/siberian-husky/</loc>
    <lastmod>2017-02-02T00:00:00.000Z</lastmod>
  </url>
  // ✂️
</urlset>
```

## robots.txt

To instruct search engines how to crawl pages on your website there is `robots.txt` file. It indicates whether certain user
agent can or cannot crawl parts of a website. These crawl instructions are specified by `disallow` and `allow` directives
for all or certain user agents.

To generate `robots.txt` install [gatsby-plugin-robots-txt](https://www.gatsbyjs.com/plugins/gatsby-plugin-robots-txt/).

```
yarn add gatsby-plugin-robots-txt
```

Update `gatsby-config.js`:

```js fileName=gatsby-config.js
// ✂️
const {
  NODE_ENV,
  SITE_URL,
  URL: NETLIFY_SITE_URL = SITE_URL,
  DEPLOY_PRIME_URL: NETLIFY_DEPLOY_URL = NETLIFY_SITE_URL,
  CONTEXT: NETLIFY_ENV = NODE_ENV,
} = process.env
// ✂️

module.exports = {
  // ✂️
  plugins: [
    // ✂️
    {
      resolve: `gatsby-plugin-robots-txt`,
      options: {
        resolveEnv: () => NETLIFY_ENV,
        env: {
          production: {
            policy: [{ userAgent: `*` }],
          },
          "branch-deploy": {
            policy: [{ userAgent: `*`, disallow: [`/`] }],
            sitemap: null,
            host: null,
          },
          "deploy-preview": {
            policy: [{ userAgent: `*`, disallow: [`/`] }],
            sitemap: null,
            host: null,
          },
        },
      },
    },
    // ✂️
  ],
}
```

With Netlify you can make sure that search engines do not have access to the development version of your website.
We can rely on `NETLIFY_ENV` environment variable. When `NETLIFY_ENV` value is `production` we allow all user
agents to access all URLs. When `NETLIFY_ENV` value is `branch-deploy` or `deploy-preview` we deny access
to all URLs and user agents.

`robots.txt` is generated in the root of your website `http://<SITE_URL>/robots.txt`. With the above configuration your
production `robots.txt` will look, like:

```
User-agent: *
Sitemap: https://gatsby-seo.netlify.app/sitemap/sitemap-index.xml
Host: https://gatsby-seo.netlify.app
```

In non production environments (`branch-deploy` or `deploy-preview`):

```
User-agent: *
Disallow: /
```
